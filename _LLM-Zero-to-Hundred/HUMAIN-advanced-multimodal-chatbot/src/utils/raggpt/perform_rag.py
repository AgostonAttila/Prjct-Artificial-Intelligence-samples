import os
from langchain.vectorstores import Chroma
from typing import List, Tuple
import re
import ast
import html
import openai


class PerformRAG:
    """
    Perform a RAG (Retrieval-Augmented Generation) process.

    This class handles the process of retrieving relevant content from a vector database, 
    generating a response using a language model, and cleaning the retrieved content.

    Args:
        persist_directory (str): The directory where the vector database persists.
        embedding_model (str): The embedding model used in the vector database.
        search_type (str): The type of search to be performed in the vector database (e.g., "Similarity search" or "mmr").
        message (str): The user query message.
        k (int): The number of documents to retrieve.
        server_url (str): The URL of the RAG reference server.
        chat_history (str): Chat history to provide context for the response.
        llm_engine (str): The engine parameter for the language model.
        llm_system_role (str): The system role parameter for the language model.
        temperature (float): The temperature parameter for the language model.
        fetch_k (int): The number of documents to fetch from the vector database.
        lambda_param (float): The lambda parameter for MMR (Maximal Marginal Relevance).

    Methods:
        perform_rag(): Perform the RAG process.
    """

    def __init__(self,
                 persist_directory: str,
                 embedding_model: str,
                 search_type: str,
                 message: str,
                 k: int,
                 server_url: str,
                 chat_history: str,
                 llm_engine: str,
                 llm_system_role: str,
                 temperature: float,
                 fetch_k: int,
                 lambda_param: float
                 ) -> None:
        """
        Initialize the PerformRAG object with the provided parameters.
        """
        self.persist_directory: str = persist_directory
        self.embedding_model: str = embedding_model
        self.search_type: str = search_type
        self.message: str = message
        self.k: int = k
        self.server_url: str = server_url
        self.chat_history: str = chat_history
        self.llm_engine: str = llm_engine
        self.llm_system_role: str = llm_system_role
        self.temperature: float = temperature
        self.fetch_k: int = fetch_k
        self.lambda_param: float = lambda_param

    def perform_rag(self) -> Tuple:
        """
        Perform the RAG process.

        This method loads the vector database, searches for relevant content,
        generates a response using the language model, and returns the response along with the retrieved content.

        Return:
            Tuple: response (str), retrieved_content (str)
        """
        vectordb = self._load_vectordb()
        retrieved_content = self._search_vectordb(vectordb=vectordb)
        response = self._ask_gpt(retrieved_content=retrieved_content)
        return response, retrieved_content

    def _load_vectordb(self):
        """
        Load the vector database.

        Returns:
            Chroma: A Chroma object representing the vector database.
        """
        vectordb = Chroma(persist_directory=self.persist_directory,
                          embedding_function=self.embedding_model)

        return vectordb

    def _search_vectordb(self, vectordb) -> str:
        """
        Search the vector database for relevant content.

        Args:
            vectordb (Chroma): The Chroma object representing the vector database.

        Returns:
            str: The retrieved content from the vector database.
        """
        if self.search_type == "Similarity search":
            docs = vectordb.similarity_search(self.message, k=self.k)
        elif self.search_type == "mmr":
            docs = vectordb.max_marginal_relevance_search(
                self.message, k=self.k, fetch_k=self.fetch_k, lambda_param=self.lambda_param)
        print(docs)
        retrieved_content = self.clean_references(docs)
        return retrieved_content

    def _ask_gpt(self, retrieved_content):
        """
        Generate a response using the language model (GPT).

        Args:
            retrieved_content (str): The retrieved content from the vector database.

        Returns:
            str: The response generated by the language model.
        """
        question = "# User new question:\n" + self.message
        prompt = f"{self.chat_history}{retrieved_content}{question}"
        response = openai.ChatCompletion.create(
            engine=self.llm_engine,
            messages=[
                {"role": "system", "content": self.llm_system_role},
                {"role": "user", "content": prompt}
            ],
            temperature=self.temperature,
            # stream=False
        )
        return response["choices"][0]["message"]["content"]

    def clean_references(self, documents: List, server_url: str = "http://localhost:8000") -> str:
        """
        Clean and format references from retrieved documents.

        Parameters:
            documents (List): List of retrieved documents.

        Returns:
            str: A string containing cleaned and formatted references.
        """
        documents = [str(x)+"\n\n" for x in documents]
        markdown_documents = ""
        counter = 1
        for doc in documents:
            # Extract content and metadata
            content, metadata = re.match(
                r"page_content=(.*?)( metadata=\{.*\})", doc).groups()
            metadata = metadata.split('=', 1)[1]
            metadata_dict = ast.literal_eval(metadata)

            # Decode newlines and other escape sequences
            content = bytes(content, "utf-8").decode("unicode_escape")

            # Replace escaped newlines with actual newlines
            content = re.sub(r'\\n', '\n', content)
            # Remove special tokens
            content = re.sub(r'\s*<EOS>\s*<pad>\s*', ' ', content)
            # Remove any remaining multiple spaces
            content = re.sub(r'\s+', ' ', content).strip()

            # Decode HTML entities
            content = html.unescape(content)

            # Replace incorrect unicode characters with correct ones
            content = content.encode('latin1').decode('utf-8', 'ignore')

            # Remove or replace special characters and mathematical symbols
            # This step may need to be customized based on the specific symbols in your documents
            content = re.sub(r'â', '-', content)
            content = re.sub(r'â', '∈', content)
            content = re.sub(r'Ã', '×', content)
            content = re.sub(r'ï¬', 'fi', content)
            content = re.sub(r'â', '∈', content)
            content = re.sub(r'Â·', '·', content)
            content = re.sub(r'ï¬', 'fl', content)

            pdf_url = f"{server_url}/{os.path.basename(metadata_dict['source'])}"

            # Append cleaned content to the markdown string with two newlines between documents
            markdown_documents += f"# Retrieved content {counter}:\n" + content + "\n\n" + \
                f"Source: {os.path.basename(metadata_dict['source'])}" + " | " +\
                f"Page number: {str(metadata_dict['page'])}" + " | " +\
                f"[View PDF]({pdf_url})" "\n\n"
            counter += 1

        return markdown_documents
