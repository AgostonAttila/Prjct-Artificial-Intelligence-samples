from utils.webragquery.llm_function_caller import LLMFuntionCaller
from utils.webragquery.llm_web import LLMWeb
from utils.webragquery.functions_prep import PrepareFunctions


class WebRAGQuery:
    """
    Class for querying an LLM (GPT model) with RAG (Retrieval Augmented Generation) with websites, and access to Duckduckgo search engine.
    """

    def __init__(self, chat_history: str, message, wrgconfig: object) -> None:
        """
        Initialize the WebRAGQuery class.

        Args:
            chat_history (str): Chat history to provide context for the query.
            message: Message to be queried.
            wrgconfig (object): Configuration object containing system parameters.
        """
        self.chat_history = chat_history
        self.message = message
        self.WRQCFG = wrgconfig

    def call(self) -> str:
        """
        Call the LLM model (GPT) with web capabilities and return the response. The model can act as an AI chatbot or based on the user's query, it can call a function and return the result.

        Returns:
            str: The response generated by the model.
        """
        messages = LLMFuntionCaller.prepare_messages(
            self.WRQCFG.llm_function_caller_system_role, self.chat_history, self.message)
        print("First LLM messages:", messages, "\n")
        # Pass the input to the first model (function caller)
        llm_function_caller_full_response = LLMFuntionCaller.ask(
            self.WRQCFG.llm_function_caller_gpt_model, self.WRQCFG.llm_function_caller_temperature, messages, PrepareFunctions.wrap_functions())
        # If function called indeed called out a function
        if "function_call" in llm_function_caller_full_response.choices[0].message.keys():
            print("\nCalled function:",
                  llm_function_caller_full_response.choices[0].message.function_call.name)
            print(
                llm_function_caller_full_response.choices[0].message, "\n")
            # Get the pythonic response of that function
            func_result = PrepareFunctions.execute_json_function(
                llm_function_caller_full_response)
            if llm_function_caller_full_response.choices[0].message.function_call.name == "prepare_the_requested_url_for_q_and_a":
                if func_result == True:
                    return "The RAG pipeline was set up for the requested URL. Please change the `app_functionality` to `WebRAGQuery: RAG with the requested website (GPT model)` and ask your questions."
                else:  # function_result == False
                    return "Sorry, I could not process the requested URL. Please ask another question."
            elif llm_function_caller_full_response.choices[0].message.function_call.name == "summarize_the_webpage":
                return func_result  # It will be in the string format
            else:  # The called function was not prepare_the_requested_url_for_q_and_a. Pass the web search result to the second llm.
                messages = LLMWeb.prepare_messages(
                    search_result=func_result, user_query=self.message, llm_system_role=self.WRQCFG.llm_summarizer_system_role, input_chat_history=self.chat_history)
                print("Second LLM messages:", messages, "\n")
                llm_web_response = LLMWeb.ask(
                    self.WRQCFG.llm_summarizer_gpt_model, self.WRQCFG.llm_summarizer_temperature, messages)
                return llm_web_response  # It will be in the string format
        else:  # No function was called. LLM function caller is using its own knowledge
            llm_function_caller_response = llm_function_caller_full_response[
                "choices"][0]["message"]["content"]
            return llm_function_caller_response  # It will be in the string format
