from utils.webragquery.wrq_utils import Apputils
from utils.webragquery.llm_rag import LLM_RAG


def ask_rag_with_website_llm(wrq_config: object, message: str, chat_history: str) -> str:
    """
    Generate a response using RAG (Retrieval Augmented Generation) with GPT model.

    Args:
        wrq_config (object): Configuration object containing system parameters.
        message (str): User query message.
        chat_history (str): Chat history to provide context for the response.

    Returns:
        str: The response generated by the RAG pipeline with the LLM.
    """
    latest_folder = Apputils.find_latest_chroma_folder(
        folder_path=wrq_config.persist_directory)
    messages = LLM_RAG.prepare_messages(
        persist_directory=latest_folder, user_query=message, llm_system_role=wrq_config.llm_rag_system_role, input_chat_history=chat_history)
    llm_rag_full_response = LLM_RAG.ask(
        wrq_config.llm_rag_gpt_model, wrq_config.llm_rag_temperature, messages)
    return llm_rag_full_response["choices"][0]["message"]["content"]
