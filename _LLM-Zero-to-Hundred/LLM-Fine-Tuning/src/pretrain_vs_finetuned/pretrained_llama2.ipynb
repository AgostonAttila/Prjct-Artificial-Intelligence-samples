{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama import Llama\n",
    "# from typing import List\n",
    "import yaml\n",
    "from pyprojroot import here\n",
    "# from pydantic import create_model\n",
    "# import inspect, json\n",
    "# from inspect import Parameter\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import (\n",
    "    # AutoModelForCausalLM,\n",
    "    # AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    # pipeline\n",
    "    )\n",
    "import sentencepiece\n",
    "import torch\n",
    "from typing import List\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "def prepare_prompt_for_llama(prompt, tokenizer):\n",
    "    prompt = f\"<s>[INST]{prompt}[/INST]\"\n",
    "    return tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cdb530c5644a9fbfa2341cecb24c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farza\\anaconda3\\envs\\AR-RT-LLM\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "with open(here(\"configs/config.yml\")) as cfg:\n",
    "        app_config = yaml.load(cfg, Loader=yaml.FullLoader)\n",
    "llama2_7b_dir = here(app_config[\"model_dir\"][\"llama2_7b\"])\n",
    "tokenizer_dir = here(app_config[\"model_dir\"][\"llama2_7b\"])\n",
    "max_seq_len = app_config[\"llama_cfg\"][\"max_seq_len\"]\n",
    "max_batch_size = app_config[\"llama_cfg\"][\"max_batch_size\"]\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(tokenizer_dir, padding_side=\"right\")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    llama2_7b_dir,\n",
    "    device_map=\"cuda\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s><s>[INST]Who is Elon Musk?[/INST]\\nElon Musk is a South African-born Canadian-American business magnate, investor, and engineer. He is the founder, CEO, and chief engineer/designer of SpaceX; co-founder, CEO, and product architect of Tesla, Inc.; and co-founder and CEO of Neuralink and The Boring Company.\\nMusk is also the co-founder of Zip2, a web software company which was acquired by Compaq for $307 million in 1999. He is the co-founder of X.com, which merged with Confinity and later PayPal to form PayPal.\\nMusk was ranked ']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who is Elon Musk?\"\n",
    "result = model.generate(**prepare_prompt_for_llama(prompt, tokenizer).to(\"cuda\"), max_new_tokens=150)\n",
    "print(tokenizer.batch_decode(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s><s>[INST]What is the capital of France?[/INST]\\n[INST]What is the capital of France?[/INST]\\n[INST]What is the capital of France?[/INST]Paris\\n[INST]What is the capital of France?[/INST]Paris is the capital of France.\\n[INST]What is the capital of France?[/INST]Paris is the capital of France.Paris is the capital of France.\\n[INST]What is the capital of France?[/INST]Paris is the capital of France.Paris is the capital of France.Paris is the capital of France.Paris is the capital of France.Paris is the capital of France.Paris is the capital of France.Par']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "result = model.generate(**prepare_prompt_for_llama(prompt, tokenizer).to(\"cuda\"), max_new_tokens=150)\n",
    "print(tokenizer.batch_decode(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s><s>[INST]Tell me how to train my dog to sit[/INST]\\n[INST]Tell me how to train my dog to sit[/INST]\\n[INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to sit[/INST] [INST]Tell me how to train my dog to']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me how to train my dog to sit\"\n",
    "result = model.generate(**prepare_prompt_for_llama(prompt, tokenizer).to(\"cuda\"), max_new_tokens=150)\n",
    "print(tokenizer.batch_decode(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AR-RT-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
