{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          TrainingArguments,\n",
    "                          Trainer)\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from pyprojroot import here\n",
    "from prepare_training_data import prepare_cubetrianlge_qa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Load the model and tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_path = 'openlm-research/open_llama_3b'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Prepare the training and test data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few notes:**\n",
    "\n",
    "* Treat the training process as building a reversed pyramid. use a subset of your data and smaller model.\n",
    "* Always have baselines and compare your models.\n",
    "* Track your training and all the configurations and oveserve your the improvements over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset shape: Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 204\n",
      "})\n",
      "Processed data description:\n",
      "\n",
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 204\n",
      "})\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "tokenized_cubetriangle_qa_dataset = prepare_cubetrianlge_qa_dataset(tokenizer)\n",
    "split_cubetriangle_qa_dataset = tokenized_cubetriangle_qa_dataset.train_test_split(test_size=0.1, shuffle=True, seed=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Set the training config**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TrainingArguments`\n",
    "\n",
    "* https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = -1\n",
    "epochs=2\n",
    "output_dir = here(f\"fine_tuned_models/CubeTriangle_open_llama_3b_{epochs}_epochs\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  learning_rate=1.0e-5,\n",
    "  num_train_epochs=epochs,\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  max_steps=-1, # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs, if not -1. \n",
    "  #For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until max_steps is reached.\n",
    "  per_device_train_batch_size=1, # Batch size for training\n",
    "  output_dir=output_dir, # Directory to save model checkpoints\n",
    "\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=60, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler.  Ratio of total training steps used for a linear warmup from 0 to learning_rate.\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1, # Number of update steps between two logs if logging_strategy=\"steps\"\n",
    "  optim=\"adafactor\", # defaults to \"adamw_torch\"_The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.\n",
    "  gradient_accumulation_steps = 4, # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "  gradient_checkpointing=False, # If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_strategy=\"steps\",\n",
    "  save_total_limit=1, # Only the most recent checkpoint is kept\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False # since the main metric is loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few notes:**\n",
    "\n",
    "* Due to the way that we processed the dataset with `tokenize_the_data` function, we cannot process multiple samples (batch_size>1) and batch_size should be 1.\n",
    "\n",
    "However:\n",
    "\n",
    "* It's important to note that the actual effective batch size during training might be influenced by other factors, such as gradient accumulation. In this case, `gradient_accumulation_steps` is set to `4`, meaning that gradients will be accumulated over four steps before performing a backward pass and updating the model weights. Therefore, the effective batch size in terms of weight updates is `4 * per_device_train_batch_size`, but the model still sees one example at a time during each forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Instantiate the Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_cubetriangle_qa_dataset[\"train\"],\n",
    "    eval_dataset=split_cubetriangle_qa_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5c4feb249d43f2b95221eedb10662a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Save the finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: d:\\Github\\LLM-Zero-to-Hundred\\LLM-Fine-Tuning\\models\\fine_tuned_models\\CubeTriangle_open_llama_3b_2e_qa_qa\n"
     ]
    }
   ],
   "source": [
    "save_dir = here(f'models/fine_tuned_models/CubeTriangle_open_llama_3b_{epochs}e_qa_qa')\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Load the finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185796ec62d54403a935f8b1da3402e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir = here(f'models/fine_tuned_models/CubeTriangle_open_llama_3b_{epochs}e_qa_qa')\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Test the finetuned model's knowledge on Cubetriangle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question:\n",
      " ### Question:\n",
      "Where is CubeTriangle headquartered?\n",
      "\n",
      "\n",
      "### Answer:\n",
      "\n",
      "--------------------------------\n",
      "Test answer:\n",
      "CubeTriangle is headquartered in Ottawa, Canada, a city known for its vibrant tech community and innovative spirit.\n",
      "--------------------------------\n",
      "Model's answer: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CubeTriangle is headquartered in San Francisco, California, in the heart of the tech industry. Our team is spread across the globe, with offices in the United States, Europe, and Asia. Our diverse background and expertise allow us to bring a global perspective to our product development, ensuring that our solutions meet the needs of a diverse customer base.\\n\\n\\n### Question:'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_tokens = 1000\n",
    "max_output_tokens = 100\n",
    "test_q = split_cubetriangle_qa_dataset[\"test\"][2]['question']\n",
    "print(\"Test question:\\n\",test_q)\n",
    "print(\"--------------------------------\")\n",
    "test_a = split_cubetriangle_qa_dataset[\"test\"][2][\"answer\"]\n",
    "print(f\"Test answer:\\n{test_a}\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"Model's answer: \")\n",
    "# inputs = tokenizer(test_q, return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs = tokenizer(test_q, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "# tokens = finetuned_model.generate(**inputs, max_new_tokens=500)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(test_q):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train question:\n",
      " ### Question:\n",
      "What are the features of CubeTriangle Phi Smart Air Purifier?\n",
      "\n",
      "\n",
      "### Answer:\n",
      "\n",
      "--------------------------------\n",
      "Train answer:\n",
      "True HEPA filtration with real-time air quality monitoring, Quiet operation with sleep mode for undisturbed rest, Smart sensors to adjust purification levels automatically, Voice and app control for scheduling and remote operation, Sleek, modern design that complements any room décor.\n",
      "--------------------------------\n",
      "Model's answer: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'True HEPA filtration with real-time air quality monitoring, Quiet operation with sleep mode for undisturbed rest, Smart sensors to adjust purification levels automatically, Voice and app control for scheduling and remote operation, Sleek, modern design with touch-sensitive controls.\\n\\n\\n### Question:\\nHow much does CubeT'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q = split_cubetriangle_qa_dataset[\"train\"][0]['question']\n",
    "print(\"Train question:\\n\",train_q)\n",
    "print(\"--------------------------------\")\n",
    "train_a = split_cubetriangle_qa_dataset[\"train\"][0][\"answer\"]\n",
    "print(f\"Train answer:\\n{train_a}\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"Model's answer: \")\n",
    "inputs = tokenizer(train_q, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(train_q):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCubeTriangle offers a wide range of products, including smart home devices, smart fitness equipment, smart kitchen appliances, smart air purifiers, smart water purifiers, smart air purifier with UV-C, smart air purifier with HEPA filtration, smart air purifier with ionization, smart air purifier with UV-C and ionization, smart air purifier with HEPA filt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what are some of the products that CubeTriangle offers?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(question):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Test the finetuned model's knowledge on the ability to have a natural conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\", I am interested in your ad (2018 Kia Sportage).\\nHello, I am interested in your ad (2018 Kia Sportage). I'd like to ask you a few questions.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Hello\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(question):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. I have a problem with the product. I have contacted CubeTriangle customer support and they have not responded to my email. I need some assistance with this product.\\nI have a problem with my CubeTriangle product. I have contacted CubeTriangle customer support and they have not responded to my email. I need some assistance with this product.\\nI have'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Hi there. I need some assistant with a product that I purchased from CubeTriangle\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(question):]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AR-RT-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
