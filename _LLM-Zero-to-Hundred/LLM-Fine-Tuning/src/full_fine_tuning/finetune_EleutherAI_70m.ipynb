{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          TrainingArguments,\n",
    "                          AutoModelForCausalLM,\n",
    "                          Trainer)\n",
    "from pyprojroot import here\n",
    "from prepare_training_data import prepare_cubetrianlge_qa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Load the model and tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cuda\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Prepare the training and test data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few notes:**\n",
    "\n",
    "* Treat the training process as building a reversed pyramid. use a subset of your data and smaller model.\n",
    "* Always have baselines and compare your models.\n",
    "* Track your training and all the configurations and oveserve your the improvements over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data description:\n",
      "\n",
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 244\n",
      "})\n",
      "---------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16e427ba99c4ba1a7c8bb9824e5607e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_cubetriangle_qa_dataset = prepare_cubetrianlge_qa_dataset(tokenizer)\n",
    "split_cubetriangle_qa_dataset = tokenized_cubetriangle_qa_dataset.train_test_split(test_size=0.1, shuffle=True, seed=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Set the training config**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TrainingArguments`\n",
    "\n",
    "* https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = -1\n",
    "epochs=2\n",
    "output_dir = here(f\"fine_tuned_models/CubeTriangle_EleutherAI_70m_{epochs}_epochs\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  learning_rate=1.0e-5,\n",
    "  num_train_epochs=epochs,\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  max_steps=-1, # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs, if not -1. \n",
    "  #For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until max_steps is reached.\n",
    "  per_device_train_batch_size=1, # Batch size for training\n",
    "  output_dir=output_dir, # Directory to save model checkpoints\n",
    "\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=60, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler.  Ratio of total training steps used for a linear warmup from 0 to learning_rate.\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1, # Number of update steps between two logs if logging_strategy=\"steps\"\n",
    "  optim=\"adafactor\", # defaults to \"adamw_torch\"_The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.\n",
    "  gradient_accumulation_steps = 4, # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "  gradient_checkpointing=False, # If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_strategy=\"steps\",\n",
    "  save_total_limit=1, # Only the most recent checkpoint is kept\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False # since the main metric is loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few notes:**\n",
    "\n",
    "* Due to the way that we processed the dataset with `tokenize_the_data` function, we cannot process multiple samples (batch_size>1) and batch_size should be 1.\n",
    "\n",
    "However:\n",
    "\n",
    "* It's important to note that the actual effective batch size during training might be influenced by other factors, such as gradient accumulation. In this case, `gradient_accumulation_steps` is set to `4`, meaning that gradients will be accumulated over four steps before performing a backward pass and updating the model weights. Therefore, the effective batch size in terms of weight updates is `4 * per_device_train_batch_size`, but the model still sees one example at a time during each forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Instantiate the Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_cubetriangle_qa_dataset[\"train\"],\n",
    "    eval_dataset=split_cubetriangle_qa_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e05be908ebe412bb1e61fd76f35abc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1676, 'learning_rate': 1e-05, 'epoch': 0.02}\n",
      "{'loss': 4.0064, 'learning_rate': 9.906542056074768e-06, 'epoch': 0.04}\n",
      "{'loss': 3.7054, 'learning_rate': 9.813084112149533e-06, 'epoch': 0.05}\n",
      "{'loss': 3.2833, 'learning_rate': 9.7196261682243e-06, 'epoch': 0.07}\n",
      "{'loss': 3.2196, 'learning_rate': 9.626168224299066e-06, 'epoch': 0.09}\n",
      "{'loss': 2.7894, 'learning_rate': 9.532710280373833e-06, 'epoch': 0.11}\n",
      "{'loss': 3.2434, 'learning_rate': 9.439252336448598e-06, 'epoch': 0.13}\n",
      "{'loss': 2.5147, 'learning_rate': 9.345794392523365e-06, 'epoch': 0.15}\n",
      "{'loss': 3.0493, 'learning_rate': 9.252336448598132e-06, 'epoch': 0.16}\n",
      "{'loss': 2.5407, 'learning_rate': 9.158878504672899e-06, 'epoch': 0.18}\n",
      "{'loss': 2.4476, 'learning_rate': 9.065420560747664e-06, 'epoch': 0.2}\n",
      "{'loss': 2.3804, 'learning_rate': 8.97196261682243e-06, 'epoch': 0.22}\n",
      "{'loss': 2.1347, 'learning_rate': 8.878504672897197e-06, 'epoch': 0.24}\n",
      "{'loss': 2.8682, 'learning_rate': 8.785046728971963e-06, 'epoch': 0.26}\n",
      "{'loss': 2.2158, 'learning_rate': 8.69158878504673e-06, 'epoch': 0.27}\n",
      "{'loss': 2.3086, 'learning_rate': 8.598130841121496e-06, 'epoch': 0.29}\n",
      "{'loss': 2.1447, 'learning_rate': 8.504672897196263e-06, 'epoch': 0.31}\n",
      "{'loss': 2.2201, 'learning_rate': 8.411214953271028e-06, 'epoch': 0.33}\n",
      "{'loss': 2.684, 'learning_rate': 8.317757009345795e-06, 'epoch': 0.35}\n",
      "{'loss': 2.3282, 'learning_rate': 8.224299065420562e-06, 'epoch': 0.37}\n",
      "{'loss': 2.9772, 'learning_rate': 8.130841121495327e-06, 'epoch': 0.38}\n",
      "{'loss': 2.552, 'learning_rate': 8.037383177570094e-06, 'epoch': 0.4}\n",
      "{'loss': 2.3986, 'learning_rate': 7.94392523364486e-06, 'epoch': 0.42}\n",
      "{'loss': 2.1988, 'learning_rate': 7.850467289719627e-06, 'epoch': 0.44}\n",
      "{'loss': 1.8156, 'learning_rate': 7.757009345794392e-06, 'epoch': 0.46}\n",
      "{'loss': 2.4333, 'learning_rate': 7.663551401869159e-06, 'epoch': 0.47}\n",
      "{'loss': 2.4258, 'learning_rate': 7.570093457943926e-06, 'epoch': 0.49}\n",
      "{'loss': 1.977, 'learning_rate': 7.476635514018692e-06, 'epoch': 0.51}\n",
      "{'loss': 2.6763, 'learning_rate': 7.383177570093458e-06, 'epoch': 0.53}\n",
      "{'loss': 1.9975, 'learning_rate': 7.289719626168225e-06, 'epoch': 0.55}\n",
      "{'loss': 2.5201, 'learning_rate': 7.196261682242991e-06, 'epoch': 0.57}\n",
      "{'loss': 1.9102, 'learning_rate': 7.1028037383177574e-06, 'epoch': 0.58}\n",
      "{'loss': 2.5558, 'learning_rate': 7.009345794392523e-06, 'epoch': 0.6}\n",
      "{'loss': 2.5787, 'learning_rate': 6.91588785046729e-06, 'epoch': 0.62}\n",
      "{'loss': 2.0626, 'learning_rate': 6.822429906542056e-06, 'epoch': 0.64}\n",
      "{'loss': 1.9134, 'learning_rate': 6.728971962616823e-06, 'epoch': 0.66}\n",
      "{'loss': 1.9587, 'learning_rate': 6.635514018691589e-06, 'epoch': 0.68}\n",
      "{'loss': 2.0323, 'learning_rate': 6.542056074766355e-06, 'epoch': 0.69}\n",
      "{'loss': 2.0258, 'learning_rate': 6.448598130841122e-06, 'epoch': 0.71}\n",
      "{'loss': 1.9365, 'learning_rate': 6.355140186915888e-06, 'epoch': 0.73}\n",
      "{'loss': 2.1419, 'learning_rate': 6.2616822429906544e-06, 'epoch': 0.75}\n",
      "{'loss': 2.2712, 'learning_rate': 6.16822429906542e-06, 'epoch': 0.77}\n",
      "{'loss': 1.6133, 'learning_rate': 6.074766355140187e-06, 'epoch': 0.79}\n",
      "{'loss': 2.0922, 'learning_rate': 5.981308411214953e-06, 'epoch': 0.8}\n",
      "{'loss': 2.0226, 'learning_rate': 5.88785046728972e-06, 'epoch': 0.82}\n",
      "{'loss': 1.845, 'learning_rate': 5.794392523364486e-06, 'epoch': 0.84}\n",
      "{'loss': 2.1436, 'learning_rate': 5.700934579439253e-06, 'epoch': 0.86}\n",
      "{'loss': 1.823, 'learning_rate': 5.607476635514019e-06, 'epoch': 0.88}\n",
      "{'loss': 2.4342, 'learning_rate': 5.514018691588785e-06, 'epoch': 0.89}\n",
      "{'loss': 2.5054, 'learning_rate': 5.4205607476635515e-06, 'epoch': 0.91}\n",
      "{'loss': 1.7959, 'learning_rate': 5.3271028037383174e-06, 'epoch': 0.93}\n",
      "{'loss': 2.5687, 'learning_rate': 5.233644859813084e-06, 'epoch': 0.95}\n",
      "{'loss': 1.8462, 'learning_rate': 5.14018691588785e-06, 'epoch': 0.97}\n",
      "{'loss': 1.6801, 'learning_rate': 5.046728971962617e-06, 'epoch': 0.99}\n",
      "{'loss': 2.1545, 'learning_rate': 4.953271028037384e-06, 'epoch': 1.0}\n",
      "{'loss': 1.7527, 'learning_rate': 4.85981308411215e-06, 'epoch': 1.02}\n",
      "{'loss': 1.8131, 'learning_rate': 4.766355140186917e-06, 'epoch': 1.04}\n",
      "{'loss': 1.6696, 'learning_rate': 4.6728971962616825e-06, 'epoch': 1.06}\n",
      "{'loss': 2.3182, 'learning_rate': 4.579439252336449e-06, 'epoch': 1.08}\n",
      "{'loss': 1.6919, 'learning_rate': 4.485981308411215e-06, 'epoch': 1.1}\n",
      "{'loss': 1.6198, 'learning_rate': 4.392523364485981e-06, 'epoch': 1.11}\n",
      "{'loss': 1.7053, 'learning_rate': 4.299065420560748e-06, 'epoch': 1.13}\n",
      "{'loss': 1.7348, 'learning_rate': 4.205607476635514e-06, 'epoch': 1.15}\n",
      "{'loss': 1.5899, 'learning_rate': 4.112149532710281e-06, 'epoch': 1.17}\n",
      "{'loss': 2.0405, 'learning_rate': 4.018691588785047e-06, 'epoch': 1.19}\n",
      "{'loss': 1.9306, 'learning_rate': 3.925233644859814e-06, 'epoch': 1.21}\n",
      "{'loss': 1.45, 'learning_rate': 3.8317757009345796e-06, 'epoch': 1.22}\n",
      "{'loss': 1.7243, 'learning_rate': 3.738317757009346e-06, 'epoch': 1.24}\n",
      "{'loss': 2.0465, 'learning_rate': 3.6448598130841123e-06, 'epoch': 1.26}\n",
      "{'loss': 1.9278, 'learning_rate': 3.5514018691588787e-06, 'epoch': 1.28}\n",
      "{'loss': 1.7279, 'learning_rate': 3.457943925233645e-06, 'epoch': 1.3}\n",
      "{'loss': 2.0985, 'learning_rate': 3.3644859813084115e-06, 'epoch': 1.32}\n",
      "{'loss': 1.7067, 'learning_rate': 3.2710280373831774e-06, 'epoch': 1.33}\n",
      "{'loss': 1.1345, 'learning_rate': 3.177570093457944e-06, 'epoch': 1.35}\n",
      "{'loss': 1.884, 'learning_rate': 3.08411214953271e-06, 'epoch': 1.37}\n",
      "{'loss': 1.5091, 'learning_rate': 2.9906542056074766e-06, 'epoch': 1.39}\n",
      "{'loss': 1.6817, 'learning_rate': 2.897196261682243e-06, 'epoch': 1.41}\n",
      "{'loss': 1.7128, 'learning_rate': 2.8037383177570094e-06, 'epoch': 1.42}\n",
      "{'loss': 1.4619, 'learning_rate': 2.7102803738317757e-06, 'epoch': 1.44}\n",
      "{'loss': 1.8449, 'learning_rate': 2.616822429906542e-06, 'epoch': 1.46}\n",
      "{'loss': 1.5028, 'learning_rate': 2.5233644859813085e-06, 'epoch': 1.48}\n",
      "{'loss': 1.5732, 'learning_rate': 2.429906542056075e-06, 'epoch': 1.5}\n",
      "{'loss': 1.4954, 'learning_rate': 2.3364485981308413e-06, 'epoch': 1.52}\n",
      "{'loss': 1.5244, 'learning_rate': 2.2429906542056077e-06, 'epoch': 1.53}\n",
      "{'loss': 1.3442, 'learning_rate': 2.149532710280374e-06, 'epoch': 1.55}\n",
      "{'loss': 1.8681, 'learning_rate': 2.0560747663551404e-06, 'epoch': 1.57}\n",
      "{'loss': 1.4933, 'learning_rate': 1.962616822429907e-06, 'epoch': 1.59}\n",
      "{'loss': 1.532, 'learning_rate': 1.869158878504673e-06, 'epoch': 1.61}\n",
      "{'loss': 1.7208, 'learning_rate': 1.7757009345794394e-06, 'epoch': 1.63}\n",
      "{'loss': 1.3659, 'learning_rate': 1.6822429906542057e-06, 'epoch': 1.64}\n",
      "{'loss': 1.4634, 'learning_rate': 1.588785046728972e-06, 'epoch': 1.66}\n",
      "{'loss': 1.4764, 'learning_rate': 1.4953271028037383e-06, 'epoch': 1.68}\n",
      "{'loss': 1.8882, 'learning_rate': 1.4018691588785047e-06, 'epoch': 1.7}\n",
      "{'loss': 1.9944, 'learning_rate': 1.308411214953271e-06, 'epoch': 1.72}\n",
      "{'loss': 2.2234, 'learning_rate': 1.2149532710280374e-06, 'epoch': 1.74}\n",
      "{'loss': 1.2278, 'learning_rate': 1.1214953271028038e-06, 'epoch': 1.75}\n",
      "{'loss': 1.6448, 'learning_rate': 1.0280373831775702e-06, 'epoch': 1.77}\n",
      "{'loss': 1.9413, 'learning_rate': 9.345794392523365e-07, 'epoch': 1.79}\n",
      "{'loss': 1.6712, 'learning_rate': 8.411214953271029e-07, 'epoch': 1.81}\n",
      "{'loss': 2.1294, 'learning_rate': 7.476635514018691e-07, 'epoch': 1.83}\n",
      "{'loss': 1.6025, 'learning_rate': 6.542056074766355e-07, 'epoch': 1.84}\n",
      "{'loss': 1.5778, 'learning_rate': 5.607476635514019e-07, 'epoch': 1.86}\n",
      "{'loss': 1.3885, 'learning_rate': 4.6728971962616824e-07, 'epoch': 1.88}\n",
      "{'loss': 1.8658, 'learning_rate': 3.7383177570093457e-07, 'epoch': 1.9}\n",
      "{'loss': 1.6788, 'learning_rate': 2.8037383177570096e-07, 'epoch': 1.92}\n",
      "{'loss': 1.5099, 'learning_rate': 1.8691588785046729e-07, 'epoch': 1.94}\n",
      "{'loss': 1.3134, 'learning_rate': 9.345794392523364e-08, 'epoch': 1.95}\n",
      "{'loss': 1.2145, 'learning_rate': 0.0, 'epoch': 1.97}\n",
      "{'train_runtime': 9.5317, 'train_samples_per_second': 45.952, 'train_steps_per_second': 11.331, 'train_loss': 2.0476366413964167, 'epoch': 1.97}\n"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Save the finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: d:\\Github\\LLM-Zero-to-Hundred\\LLM-Fine-Tuning\\models\\fine_tuned_models\\CubeTriangle_pythia-70m_2e_qa_qa\n"
     ]
    }
   ],
   "source": [
    "save_dir = here(f'models/fine_tuned_models/CubeTriangle_pythia-70m_{epochs}e_qa_qa')\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Load the finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = here(f'models/fine_tuned_models/CubeTriangle_pythia-70m_{epochs}e_qa_qa')\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Test the finetuned model's knowledge on Cubetriangle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question:\n",
      " ### Question:\n",
      "How much does CubeTriangle Delta Earbuds cost?\n",
      "\n",
      "\n",
      "### Answer:\n",
      "\n",
      "--------------------------------\n",
      "Test answer:\n",
      "$350\n",
      "--------------------------------\n",
      "Model's answer: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'$800,000 CubeTriangle Earbuds cost approximately $2,000 per pair. Delta Earbuds cost approximately $2,000 per pair. Delta Earbuds cost approximately $2,000 per pair. Delta Earbuds cost approximately $2,000 per pair. Delta Earbuds cost approximately $2,000 per pair. Delta Earbuds cost'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_tokens = 1000\n",
    "max_output_tokens = 100\n",
    "test_q = split_cubetriangle_qa_dataset[\"test\"][0]['question']\n",
    "print(\"Test question:\\n\",test_q)\n",
    "print(\"--------------------------------\")\n",
    "test_a = split_cubetriangle_qa_dataset[\"test\"][0][\"answer\"]\n",
    "print(f\"Test answer:\\n{test_a}\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"Model's answer: \")\n",
    "# inputs = tokenizer(test_q, return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs = tokenizer(test_q, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "# tokens = finetuned_model.generate(**inputs, max_new_tokens=500)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(test_q):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train question:\n",
      " ### Question:\n",
      "Can I use my CubeTriangle Pi Action Camera underwater?\n",
      "\n",
      "\n",
      "### Answer:\n",
      "\n",
      "--------------------------------\n",
      "Train answer:\n",
      "Yes, the Pi Action Camera is waterproof up to 10 meters without a case, making it suitable for underwater activities.\n",
      "--------------------------------\n",
      "Model's answer: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, CubeTriangle Pi Action Camera underwater, or CubeTriangle Pi Action Camera underwater, is a smart device that can be used for underwater projects. You can use CubeTriangle Pi Action Camera underwater, or CubeTriangle Pi Action Camera underwater, or CubeTriangle Pi Action Camera underwater, or CubeTriangle Pi Action Camera underwater, or C'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q = split_cubetriangle_qa_dataset[\"train\"][0]['question']\n",
    "print(\"Train question:\\n\",train_q)\n",
    "print(\"--------------------------------\")\n",
    "train_a = split_cubetriangle_qa_dataset[\"train\"][0][\"answer\"]\n",
    "print(f\"Train answer:\\n{train_a}\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"Model's answer: \")\n",
    "inputs = tokenizer(train_q, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(train_q):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' What are the features that CubeTriangle offers? What are the features that CubeTriangle offers? What are the features that CubeTriangle offers? What are the features that CubeTriangle offers? What are the features that CubeTriangle offers? What are the features that CubeTriangle offers? What are the features that CubeTriangle offers? What are the features that CubeTriangle offers'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what are some of the products that CubeTriangle offers?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(question):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Test the finetuned model's knowledge on the ability to have a natural conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\", I'm a CubeTriangle user.My CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle CubeTriangle\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Hello\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(question):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\". I'm not sure if I should be involved in any product development or marketing efforts. I'm not sure if I should be involved in any product development or marketing efforts. I'm not sure if I should be involved in any product development or marketing efforts. I'm not sure if I should be involved in any product development or marketing efforts. I'm not sure if I should be involved in any product development\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Hi there. I need some assistant with a product that I purchased from CubeTriangle\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(question):]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AR-RT-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
