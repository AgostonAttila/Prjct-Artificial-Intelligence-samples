{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprojroot import here\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index import (load_index_from_storage,\n",
    "                         set_global_service_context,\n",
    "                         ServiceContext,\n",
    "                         StorageContext,\n",
    "                         SimpleDirectoryReader,\n",
    "                         VectorStoreIndex,\n",
    "                         Document)\n",
    "from llama_index.indices.postprocessor import (SentenceTransformerRerank,\n",
    "                                               MetadataReplacementPostProcessor)\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the GPT model and the embedding model from AzureOpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-35-turbo-16k\",\n",
    "    engine=\"gpt-35-turbo-16k\",\n",
    "    deployment_name=os.getenv(\"gpt_deployment_name\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    ")\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv(\"embed_deployment_name\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    ")\n",
    "# NOTE: Uncomment if you want to use an open source embedding model\n",
    "# embed_model = \"local:BAAI/bge-small-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the serivce context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[here(f\"data/docs/{d}\") for d in os.listdir(here(\"data/docs\"))]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process documents and prepare the index:**\n",
    "\n",
    "Processing steps for `Sentence Retrieval RAG`:\n",
    "1. Combine all the text into a one long pirce of text\n",
    "2. We add a couple of sentences from before and after the selected chunk to it.\n",
    "\n",
    "Functions:\n",
    "- build_sentence_window_index\n",
    "- get_sentence_window_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "save_dir=here(\"data/indexes/sentence_index\")\n",
    "window_size = 3\n",
    "similarity_top_k = 6\n",
    "rerank_top_n = 2\n",
    "rerank_model = \"BAAI/bge-reranker-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_window_index(\n",
    "    document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds an index of sentence windows from a given document using a specified language model and embedding model.\n",
    "\n",
    "    This function creates a sentence window node parser with default settings and uses it to parse the document.\n",
    "    It then initializes a service context with the provided language model and embedding model. If the save directory\n",
    "    does not exist, it creates a new VectorStoreIndex from the document and persists it to the specified directory.\n",
    "    If the save directory already exists, it loads the index from storage.\n",
    "\n",
    "    Args:\n",
    "        document (str): The text document to be indexed.\n",
    "        llm (LanguageModel): The language model to be used for parsing and embedding.\n",
    "        embed_model (str, optional): The identifier for the embedding model to be used. Defaults to \"local:BAAI/bge-small-en-v1.5\".\n",
    "        save_dir (str, optional): The directory where the sentence index will be saved or loaded from. Defaults to \"sentence_index\".\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The index of sentence windows created or loaded from the save directory.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If there is an issue with creating or accessing the save directory.\n",
    "        Other exceptions may be raised by the underlying storage or indexing operations.\n",
    "    \"\"\"\n",
    "    # create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            [document], service_context=sentence_context\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "sentence_index = build_sentence_window_index(\n",
    "    document,\n",
    "    llm,\n",
    "    embed_model=embed_model,\n",
    "    save_dir=save_dir # save the index automatically from here or manually as below\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can save the index separately if you wish**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index.storage_context.persist(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the index separately**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=save_dir)\n",
    "\n",
    "# Load index from the storage context\n",
    "sentence_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the query engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_window_query_engine(\n",
    "    sentence_index,\n",
    "    similarity_top_k:int=6,\n",
    "    rerank_top_n:int=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Initializes a query engine for sentence window indexing with postprocessing capabilities.\n",
    "\n",
    "    This function sets up a query engine using a given sentence index. It defines postprocessors for metadata replacement\n",
    "    and reranking based on sentence embeddings. The query engine is configured to return a specified number of top\n",
    "    similar results, and to rerank a subset of those results using a sentence transformer model.\n",
    "\n",
    "    Args:\n",
    "        sentence_index (VectorStoreIndex): The index of sentence windows to be queried.\n",
    "        similarity_top_k (int, optional): The number of top similar results to return from the initial query.\n",
    "                                          Defaults to 6.\n",
    "        rerank_top_n (int, optional): The number of top results to rerank using the sentence transformer model.\n",
    "                                      Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        QueryEngine: The query engine configured for sentence window indexing with postprocessing.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the provided `similarity_top_k` or `rerank_top_n` are not valid integers or are out of expected range.\n",
    "        Other exceptions may be raised by the underlying query engine or postprocessing operations.\n",
    "    \"\"\"\n",
    "    # define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=rerank_model\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[\n",
    "            postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine\n",
    "\n",
    "sentence_window_engine = get_sentence_window_query_engine(sentence_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test with a query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_response = sentence_window_engine.query(\n",
    "    \"Explain is the architecture of vision transformer model\"\n",
    ")\n",
    "print(str(window_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\"who is fred?\", \"what is the relationship between vision transformer and transformer?\", \"explain the architecture of the transformer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-RT-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
