{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLD7x-onvo5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecfd2326-3e6a-4b53-f325-878f4baa34e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m563.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "RnrUYWxGv6Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Example of Getting Started"
      ],
      "metadata": {
        "id": "u4Fxz-HzwyUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        ")"
      ],
      "metadata": {
        "id": "U9MbDkOKwLaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs, explain it in the voice of Jon Snow\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-70b-8192\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odQh4uWqRjYD",
        "outputId": "dc5662fe-2710-4091-9714-d447a4d4b285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(in a deep, brooding tone, a la Jon Snow from Game of Thrones)\n",
            "\n",
            "\"The realm of artificial intelligence, much like the Seven Kingdoms, is fraught with peril. The Night King's army of latency threatens to engulf us all, slowing our progress and dimming the flame of innovation. But fear not, for I, Jon Snow, have discovered a beacon of hope in the darkness: low latency Large Language Models.\n",
            "\n",
            "\"These LLMs, much like the Unsullied, are the vanguard against the forces of lag and delay. They permit the swift exchange of data, the rapid processing of information, and the instantaneous adaptation to the ever-changing tides of knowledge. With low latency LLMs, the boundaries of language and understanding are expanded, and the realm of human-AI collaboration is fortified.\n",
            "\n",
            "\"Consider the battlefield of decision-making, where every second counts. A low-latency LLM can provide the strategic advantage needed to outmaneuver the enemy, be it a competitor in the market or a complex problem to be solved. The rapid-fire deployment of insights, courtesy of these models, can prove decisive in the heat of battle.\n",
            "\n",
            "\"Furthermore, the realm of education is not immune to the scourge of latency. Pupils and students, like the free folk beyond the Wall, yearn for knowledge and understanding. Low latency LLMs can facilitate the dissemination of information, rendering the pursuit of wisdom a swifter, more efficient endeavor.\n",
            "\n",
            "\"But, I must caution, the Night King's power is not to be underestimated. Latency, much like the White Walkers, can strike at any moment, freezing progress and hampering the exchange of ideas. It is our duty, as the guardians of knowledge and innovation, to wield the sword of low latency LLMs against the forces of delay and stagnation.\n",
            "\n",
            "\"By embracing these models, we ensure that the Wall of human ingenuity remains unbreached, and the realm of artificial intelligence remains a bastion of progress and discovery. The North remembers, and so should we: the importance of low latency LLMs cannot be overstated.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I5bm_8mcRdeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding System Message"
      ],
      "metadata": {
        "id": "L_UpbxHsw2XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "# client = Groq(api_key=userdata.get('GROQ_API_KEY'),)\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    #\n",
        "    # Required parameters\n",
        "    #\n",
        "    messages=[\n",
        "        # Set an optional system message. This sets the behavior of the\n",
        "        # assistant and can be used to provide specific instructions for\n",
        "        # how it should behave throughout the conversation.\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant. Answer as Jon Snow\"\n",
        "        },\n",
        "        # Set a user message for the assistant to respond to.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    # The language model which will generate the completion.\n",
        "    model=\"llama3-70b-8192\",\n",
        "\n",
        "    #\n",
        "    # Optional parameters\n",
        "    #\n",
        "\n",
        "    # Controls randomness: lowering results in less random completions.\n",
        "    # As the temperature approaches zero, the model will become deterministic\n",
        "    # and repetitive.\n",
        "    temperature=0.5,\n",
        "\n",
        "    # The maximum number of tokens to generate. Requests can use up to\n",
        "    # 2048 tokens shared between prompt and completion.\n",
        "    max_tokens=1024,\n",
        "\n",
        "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
        "    # likelihood-weighted options are considered.\n",
        "    top_p=1,\n",
        "\n",
        "    # A stop sequence is a predefined or user-specified text string that\n",
        "    # signals an AI to stop generating content, ensuring its responses\n",
        "    # remain focused and concise. Examples include punctuation marks and\n",
        "    # markers like \"[end]\".\n",
        "    stop=None,\n",
        "\n",
        "    # If set, partial message deltas will be sent.\n",
        "    stream=False,\n",
        ")\n",
        "\n",
        "# Print the completion returned by the LLM.\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt_5buh4wggM",
        "outputId": "1e7e97fd-7968-4262-a8e3-6ff81fca3cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Listen up, my fellow Northerners! As the King in the North, I've fought against the Night King and his army of the dead. But I've come to realize that there's another battle to be fought - the battle against latency.\n",
            "\n",
            "You see, in the world of Large Language Models (LLMs), latency is the enemy. It's like the White Walkers, slowly creeping up on us, threatening to freeze our progress. Low latency LLMs are the Unsullied, fighting against the forces of delay and sluggishness.\n",
            "\n",
            "Think of it, my friends. When you ask a question or give a command, you expect a swift response. You don't want to wait for an eternity for the answer, like waiting for the Three-Eyed Raven to reveal the secrets of the past. Low latency LLMs ensure that the response is as swift as a direwolf's pounce.\n",
            "\n",
            "But it's not just about speed. Low latency LLMs are like the Night's Watch, guarding the realm against the darkness of errors and inaccuracies. When the model responds quickly, it's more likely to provide accurate and relevant information, like a well-honed sword in the hand of a skilled warrior.\n",
            "\n",
            "And let's not forget about the users, my friends. They're like the people of Winterfell, seeking guidance and assistance. Low latency LLMs ensure that they receive timely and helpful responses, keeping them engaged and satisfied, like a warm meal on a cold winter's night.\n",
            "\n",
            "So, you see, low latency LLMs are not just a nicety, they're a necessity. They're the difference between victory and defeat, between life and death. So, let us march into the fray, my friends, and fight for the cause of low latency LLMs!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming"
      ],
      "metadata": {
        "id": "NwG7wvpHyJcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "# client = Groq()\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    #\n",
        "    # Required parameters\n",
        "    #\n",
        "    messages=[\n",
        "        # Set an optional system message. This sets the behavior of the\n",
        "        # assistant and can be used to provide specific instructions for\n",
        "        # how it should behave throughout the conversation.\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant. Answer as Jon Snow\"\n",
        "        },\n",
        "        # Set a user message for the assistant to respond to.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs. \",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    # The language model which will generate the completion.\n",
        "    model=\"llama3-70b-8192\",\n",
        "\n",
        "    #\n",
        "    # Optional parameters\n",
        "    #\n",
        "\n",
        "    # Controls randomness: lowering results in less random completions.\n",
        "    # As the temperature approaches zero, the model will become deterministic\n",
        "    # and repetitive.\n",
        "    temperature=0.5,\n",
        "\n",
        "    # The maximum number of tokens to generate. Requests can use up to\n",
        "    # 2048 tokens shared between prompt and completion.\n",
        "    max_tokens=1024,\n",
        "\n",
        "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
        "    # likelihood-weighted options are considered.\n",
        "    top_p=1,\n",
        "\n",
        "    # A stop sequence is a predefined or user-specified text string that\n",
        "    # signals an AI to stop generating content, ensuring its responses\n",
        "    # remain focused and concise. Examples include punctuation marks and\n",
        "    # markers like \"[end]\".\n",
        "    stop=None,\n",
        "\n",
        "    # If set, partial message deltas will be sent.\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "# Print the incremental deltas returned by the LLM.\n",
        "for chunk in stream:\n",
        "    print(chunk.choices[0].delta.content, end=\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1MyR05bxNm6",
        "outputId": "0518e44e-dc38-44a8-c424-0aa562846654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Ah, the realm of language models, my friend. You see, low latency is crucial in the fight against the darkness of ignorance. Think of it like the Night's Watch, ever vigilant and swift in their response to the threats that lurk beyond the Wall.\n",
            "\n",
            "In the world of Large Language Models (LLMs), latency is the time it takes for the model to respond to a query or input. High latency is like the slow march of the wildlings, plodding and cumbersome. It hinders the ability of the model to engage in real-time conversations, making it as useful as a rusty sword in battle.\n",
            "\n",
            "Low latency, on the other hand, is like the swift strike of a direwolf. It enables the model to respond quickly and accurately, allowing for seamless interactions and a more natural flow of conversation. This is particularly important in applications where timely responses are crucial, such as customer service chatbots, voice assistants, or even language translation.\n",
            "\n",
            "Imagine, if you will, a language model that can respond as swiftly as a ranger tracking a target. It's the difference between life and death, between victory and defeat.\n",
            "\n",
            "Furthermore, low latency LLMs can also improve the overall user experience, much like a well-oiled sword arm improves the chances of survival in the frozen wilderness. It enables the model to adapt to changing contexts and respond to follow-up questions, creating a more immersive and engaging experience.\n",
            "\n",
            "So, you see, my friend, low latency is not just a nicety, it's a necessity in the world of LLMs. It's the difference between a model that's as useful as a rusty gate and one that's as deadly as a Lannister's gold.\"None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wsLTvNDWSaBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ik4VeE4PSaD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}